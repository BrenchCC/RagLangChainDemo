import os
import re
import json
import asyncio
import argparse
import uuid
import time
import logging
from contextlib import asynccontextmanager
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
from langchain_openai import ChatOpenAI
# éƒ¨ç½²REST APIç›¸å…³
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
# å‘é‡æ•°æ®åº“chromaç›¸å…³
from langchain_chroma import Chroma
# openaiçš„å‘é‡æ¨¡å‹
from langchain_openai import OpenAIEmbeddings
# RAGç›¸å…³
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, ConfigurableFieldSpec
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import SQLChatMessageHistory # å¤„ç†å’Œå­˜å‚¨å¯¹è¯å†å²
from torchvision.transforms.v2.functional import ten_crop_image

# è®¾ç½®langsmithå˜é‡
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "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"


# è®¾ç½®æ—¥å¿—æ¨¡ç‰ˆ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

MODEL = None
EMBEDDINGS = None
VECTORSTORE = None
SYSTEM_PROMPT = None
CHAIN = None
PORT = 8002

PROMPT_TEMPLATE_TXT = "prompts/prompt.txt"

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_url", default = "https://dashscope.aliyuncs.com/compatible-mode/v1", help = "OpenAI API URL")
    parser.add_argument("--api_key", default = "", help = "OpenAI API Key")
    parser.add_argument("--chat_model", default = "qwen-plus", help = "OpenAI Chat Model")
    parser.add_argument("--embedding_model", default = "text-embedding-v2", help = "OpenAI Chat Model ID")
    parser.add_argument("--chromadb_dir", default = "chromaDB", help = "Chromadb Directory")
    parser.add_argument("--chromadb_collection", default = "demov1", help = "Chroma Collection Name")

    return parser.parse_args()

# å®šä¹‰Messageç±»
class Message(BaseModel):
    role: str
    content: str

# å®šä¹‰ChatCompletionRequestç±»
class ChatCompletionRequest(BaseModel):
    messages: List[Message]
    stream: Optional[bool] = False
    user_id: Optional[str] = None
    conversation_id: Optional[str] = None

# å®šä¹‰ChatCompletionResponseChoiceç±»
class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: Message
    finish_reason: Optional[str] = None

# å®šä¹‰ChatCompletionResponseç±»
class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory = lambda: int(time.time()))
    choices: List[ChatCompletionResponseChoice]
    system_fingerprint: Optional[str] = None

# è·å–å¯¹è¯å†å²
def get_session_history(user_id: str, conversation_id: str):
    return SQLChatMessageHistory(f"{user_id}--{conversation_id}, sqlite:///memory/memory.db")

# è·å–promptåœ¨chainä¸­ä¼ é€’çš„promptæœ€ç»ˆçš„å†…å®¹
def get_prompt(prompt):
    logger.info(f"æœ€åç»™åˆ°LLMçš„promptçš„å†…å®¹: {prompt}")
    return prompt

def format_search_result(search_result):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ \n{2, }å°†è¾“å…¥çš„responseæŒ‰ç…§ä¸¤ä¸ªæˆ–æ›´å¤šçš„è¿ç»­æ¢è¡Œç¬¦è¿›è¡Œåˆ†å‰²ã€‚è¿™æ ·å¯ä»¥å°†æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªæ®µè½ï¼Œæ¯ä¸ªæ®µè½ç”±è¿ç»­çš„éç©ºè¡Œç»„æˆ
    paragraphs = re.split(r'\n{2,}', search_result)

    # ç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ ¼å¼åŒ–åçš„æ®µè½
    formatted_paragraphs = []

    for paragraph in paragraphs:
        if "```" in paragraph:
            # æ ¹æ®æ®µè½æŒ‰ç…§```åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œä»£ç ğŸ’¨å’Œæ™®é€šæ–‡æœ¬äº¤æ›¿å‡ºç°
            parts = paragraph.split("```")
            for i, part in enumerate(parts):
                if i % 2 == 1:
                    # ä»£ç å—ï¼Œç”¨æ¢è¡Œç¬¦å’Œ```åŒ…å›´ï¼Œå¹¶å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
                    parts[i] = f"\n```\n{part.strip()}\n```\n"
            paragraph = " ".join(parts)
        #
        # else:
        #     # å¦åˆ™ï¼Œå°†å¥å­ä¸­çš„å¥ç‚¹åé¢çš„ç©ºæ ¼æ›¿æ¢ä¸ºæ¢è¡Œç¬¦ï¼Œä»¥ä¾¿å¥å­ä¹‹é—´æœ‰æ˜ç¡®çš„åˆ†éš”
        #     paragraph = paragraph.replace('.', '.\n')
        formatted_paragraphs.append(paragraph.strip())

    return "\n\n".join(formatted_paragraphs)

@asynccontextmanager
async def lifespan(app: FastAPI):
    global MODEL, EMBEDDINGS, VECTORSTORE, SYSTEM_PROMPT, CHAIN, PORT

    parser = parse_args()
    base_url = parser.base_url
    api_key = parser.api_key
    chat_model = parser.chat_model
    embedding_model = parser.embedding_model
    chromadb_dir = parser.chromadb_dir
    chromadb_collection = parser.chromadb_collection

    try:
        logger.info("Initializing LLM Model, Chroma DB; Extract System Prompt; Define Chain")
        # æ¨¡å‹åˆå§‹åŒ–
        MODEL = ChatOpenAI(
            base_url = base_url,
            api_key = api_key,
            model = chat_model,
            temperature = 0.8,
            top_p = 0.9,
            timeout = None,
            max_retries = 3
        )

        # embedding æ¨¡å‹åˆå§‹åŒ–
        EMBEDDINGS = OpenAIEmbeddings(
            base_url = base_url,
            api_key = api_key,
            model = embedding_model,
            check_embedding_ctx_length = False
        )

        # åˆå§‹åŒ–ChromaDBå¯¹è±¡
        VECTORSTORE = Chroma(
            persist_directory = chromadb_dir,
            collection_name = chromadb_collection,
            embedding_function = EMBEDDINGS
        )

        # è·å–system_prompt
        system_prompt = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT)
        logger.info(f"system_prompt: {system_prompt}")
        SYSTEM_PROMPT = ChatPromptTemplate.from_messages(
            [
                ("system", "ä½ æ˜¯ä¸€ä¸ªé’ˆå¯¹å¥åº·æ¡£æ¡ˆè¿›è¡Œé—®ç­”çš„æœºå™¨äººã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"),
                MessagesPlaceholder(variable_name = "history"),
                ("human",str(system_prompt.template))
            ]
        )

        # å®šä¹‰chain
        # å°†RAGæ£€ç´¢æ”¾åˆ°LangChainçš„LCELçš„chainä¸­æ‰§è¡Œ
        # è¿™æ®µä»£ç æ˜¯ä½¿ç”¨Langchainæ¡†æ¶ä¸­çš„`as_retriever`æ–¹æ³•åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨å¯¹è±¡
        # LangChain VectorStoreå¯¹è±¡ä¸æ˜¯ Runnable çš„å­ç±»ï¼Œå› æ­¤æ— æ³•é›†æˆåˆ°LangChainçš„LCELçš„chainä¸­
        # LangChain Retrieversæ˜¯Runnableï¼Œå®ç°äº†ä¸€ç»„æ ‡å‡†æ–¹æ³•å¯é›†æˆåˆ°LCELçš„chainä¸­
        # `vectorstore`æ˜¯ä¸€ä¸ªå‘é‡å­˜å‚¨å¯¹è±¡ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æœ¬æ•°æ®
        # `as_retriever`æ–¹æ³•å°†å‘é‡å­˜å‚¨å¯¹è±¡è½¬æ¢ä¸ºä¸€ä¸ªæ£€ç´¢å™¨å¯¹è±¡ï¼Œè¯¥å¯¹è±¡å¯ä»¥ç”¨äºæœç´¢ä¸ç»™å®šæŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬
        # `search_type`å‚æ•°è®¾ç½®ä¸º"similarity"ï¼Œè¡¨ç¤ºä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢ç®—æ³•
        # `search_kwargs`å‚æ•°æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æœç´¢ç®—æ³•çš„å‚æ•°ï¼Œè¿™é‡Œçš„`k`å‚æ•°è®¾ç½®ä¸º5ï¼Œè¡¨ç¤ºåªè¿”å›ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„5ä¸ªç»“æœ
        # retriever = VECTORSTORE.as_retriever(
        #     search_type = "similarity",
        #     search_kwargs = {"k": 5}
        # )

        CHAIN = SYSTEM_PROMPT | get_prompt | MODEL
        logger.info(f"Initializeing IS Done")

        #  å¤„ç†å¸¦æœ‰æ¶ˆæ¯å†å²Chain  å°†å¯è¿è¡Œçš„é“¾ä¸æ¶ˆæ¯å†å²è®°å½•åŠŸèƒ½ç»“åˆ
        # RunnableWithMessageHistoryå…è®¸åœ¨è¿è¡Œé“¾æ—¶æºå¸¦æ¶ˆæ¯å†å²
        # å®ä¾‹åŒ–çš„with_message_historyæ˜¯ä¸€ä¸ªé…ç½®äº†æ¶ˆæ¯å†å²çš„å¯è¿è¡Œå¯¹è±¡ï¼Œä½¿ç”¨get_session_historyæ¥è·å–å†å²è®°å½•
        # ConfigurableFieldSpecå®šä¹‰äº†ç”¨æˆ·IDå’Œä¼šè¯IDçš„é…ç½®å­—æ®µï¼Œä½¿å¾—è¿™äº›å­—æ®µåœ¨è¿è¡Œæ—¶å¯ä»¥è¢«åŠ¨æ€ä¼ é€’
        with_message_history = RunnableWithMessageHistory(
            CHAIN,
            get_session_history,
            input_messages_key = "query",
            history_messages_key = "history",
            history_factory_config = [
                ConfigurableFieldSpec(
                    id = "user_id",
                    annotation = str,
                    name = "USER ID",
                    description = "Unique identifier for the user.",
                    default = "",
                    is_shared =True
                ),
                ConfigurableFieldSpec(
                    id = "conversation_id",
                    annotation = str,
                    name = "CONVERSATION ID",
                    description = "Unique identifier for the conversation.",
                    default = "",
                    is_shared =True
                )
            ]
        )
    except Exception as e:
        logger.error(f"Error in initialization: {str(e)}")
        # raise å…³é”®å­—é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ç¡®ä¿ç¨‹åºä¸ä¼šåœ¨é”™è¯¯çŠ¶æ€ä¸‹ç»§ç»­è¿è¡Œ
        raise

    # yield å…³é”®å­—å°†æ§åˆ¶æƒäº¤è¿˜ç»™FastAPIæ¡†æ¶ï¼Œä½¿åº”ç”¨å¼€å§‹è¿è¡Œ
    # åˆ†éš”äº†å¯åŠ¨å’Œå…³é—­çš„é€»è¾‘ã€‚åœ¨yield ä¹‹å‰çš„ä»£ç åœ¨åº”ç”¨å¯åŠ¨æ—¶è¿è¡Œï¼Œyield ä¹‹åçš„ä»£ç åœ¨åº”ç”¨å…³é—­æ—¶è¿è¡Œ
    yield
    logger.info("Close Server Now...")

# lifespan å‚æ•°ç”¨äºåœ¨åº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸçš„å¼€å§‹å’Œç»“æŸæ—¶æ‰§è¡Œä¸€äº›åˆå§‹åŒ–æˆ–æ¸…ç†å·¥ä½œ
app = FastAPI(lifespan = lifespan)


# POSTè¯·æ±‚æ¥å£ï¼Œä¸å¤§æ¨¡å‹è¿›è¡ŒçŸ¥è¯†é—®ç­”
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    if not MODEL or not EMBEDDINGS or not VECTORSTORE or not CHAIN or not SYSTEM_PROMPT:
        logger.info("Server is not be initialized.")
        raise HTTPException(status_code = 500, detail = "Server is not initialized.")
    try:
        logger.info(f"Get Chat Request from User: {request}")
        user_query = request.messages[-1].content
        logger.info(f"User Query: {user_query}")

        retriever = VECTORSTORE.similarity_search(
            query = user_query,
            k = 3
        )

        # è°ƒç”¨CHAINæŸ¥è¯¢
        result = with_message_history.invoke(
            {
                "query": user_query,
                "context": retriever
            },
            config = {
                "configurable":{
                    "user_id": request.user_id,
                    "conversation_id": request.conversation_id,
                }
            }
        )

        formatted_result = str(format_search_result(result.content))
        logger.info(f"Formatted Search Result: {formatted_result}")

        if request.stream:
            async def stream_generate():
                chunk_id = f"chatcmpl-{uuid.uuid4().hex}"
                lines = formatted_result.split("\n")

                for idx, line in enumerate(lines):
                    chunk = {
                        "id": chunk_id,
                        "object": "chat.comleiton.chunk",
                        "created": int(time.time()),
                        "choices": [
                            {
                                "index": 0,
                                "delta": {"content": line + '\n'},
                                # if i > 0 else {"role": "assistant", "content": ""},
                                "finish_reason": None
                            }
                        ]
                    }

                    yield f"{json.dumps(chunk)}\n"

                    await asyncio.sleep(0.3)
                final_chunk = {
                    "id": chunk_id,
                    "object": "chat.comleiton.chunk",
                    "created": int(time.time()),
                    "choices": [
                        {
                            "index": 0,
                            "delte": {},
                            "finish_reason": "stop"
                        }
                    ]
                }
                yield f"{json.dumps(final_chunk)}\n"
            return StreamingResponse(stream_generate(), media_type = "text/event-stream")
        else:
            response = ChatCompletionResponse(
                choices=[
                    ChatCompletionResponseChoice(
                        index=0,
                        message=Message(role = "assistant", content = formatted_result),
                        finish_reason="stop"
                    )
                ]
            )
            logger.info(f"å‘é€å“åº”å†…å®¹: \n{response}")
            # è¿”å›fastapi.responsesä¸­JSONResponseå¯¹è±¡
            # model_dump()æ–¹æ³•é€šå¸¸ç”¨äºå°†Pydanticæ¨¡å‹å®ä¾‹çš„å†…å®¹è½¬æ¢ä¸ºä¸€ä¸ªæ ‡å‡†çš„Pythonå­—å…¸ï¼Œä»¥ä¾¿è¿›è¡Œåºåˆ—åŒ–
            return JSONResponse(content=response.model_dump())
    except Exception as e:
        logger.error(f"Error in processing the Chat Session:\n\n {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    logger.info(f"Start the Server in Port{PORT}")
    uvicorn.run(app, host="0.0.0.0", port = PORT)